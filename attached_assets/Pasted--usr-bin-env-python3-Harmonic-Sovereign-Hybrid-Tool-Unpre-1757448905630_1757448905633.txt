#!/usr/bin/env python3
"""
Harmonic Sovereign Hybrid Tool
==============================

Unprecedented, dependency-light, and battle-hardened doc-grounded QA engine that
runs **local-first** with optional Gemini escalation, and exposes three ways to use it:

  1) **CLI (batch/interactive)** — quick questions and answers
  2) **JSON-RPC over stdin/stdout** — programmatic control from any process
  3) **Module API** — import and call `Tool.ask({...})`

It ingests **.txt** and **.json** (flattened) knowledge bases, includes a robust
local retriever (BM25-ish + cosine TF) with **citations & confidence**, and
optionally escalates to Gemini (if installed + keyed) using retrieved context.

It’s designed to be swapped with a sovereign backend (WSM/HA) via the `Backend`
interface — just implement `ask_with_context()` and plug it in.

No third-party deps required. Includes a comprehensive test suite (`--self-test`).

CLI Examples
------------
  python harmonic_sovereign_hybrid_tool.py --kb /mnt/data/hodge_data_haqhml.json \
      --question "List any dimensions or indices mentioned." --question "What is a CICY?"

  python harmonic_sovereign_hybrid_tool.py your_doc.txt --no-strict \
      --question "Summarize the main ideas." --backend hybrid

JSON-RPC Example
----------------
  echo '{"method":"ask","params":{"question":"What is Rayleigh scattering?"}}' \
  | python harmonic_sovereign_hybrid_tool.py --jsonrpc --kb demo.txt

Outputs a JSON object with answer, citations, and confidence.

"""
import argparse
import json
import math
import os
import re
import sys
import time
import unittest
from dataclasses import dataclass
from typing import Counter as CounterType, Dict, List, Optional, Tuple, Any
from collections import Counter

# --------------------------
# Safe error printing helper
# --------------------------

def eprint(*args) -> None:
    msg = " ".join(str(a) for a in args)
    try:
        if hasattr(sys, "stderr") and sys.stderr:
            sys.stderr.write(msg + "\n")
            return
    except Exception:
        pass
    try:
        os.write(2, (msg + "\n").encode("utf-8", "replace"))
    except Exception:
        print("[stderr] " + msg)


# ------------------------------------
# Optional Gemini import (best effort)
# ------------------------------------
HAS_GENAI = False
try:
    import google.generativeai as genai  # type: ignore
    HAS_GENAI = True
except Exception:
    HAS_GENAI = False

STRICT_INSTRUCTION = (
    "You are a helpful assistant that answers questions ONLY using the provided document/context. "
    "If the answer is not there, reply: 'I cannot find the answer in the provided document.' "
)

RELAXED_INSTRUCTION = (
    "You are a helpful assistant. Prefer the provided context for answers. If something is missing, "
    "you may use general knowledge, but explicitly say it is outside the document."
)

NOT_FOUND_LINE = "I cannot find the answer in the provided document."

# -----------------
# Common utilities
# -----------------

def load_text_from_file(file_path: str) -> str:
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


def chunk_text(text: str, chunk_size: int = 12000, overlap: int = 400) -> List[str]:
    text = text.strip()
    if not text:
        return []
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunks.append(text[start:end])
        start = max(0, end - overlap)
    return chunks


def flatten_json(obj, prefix="") -> List[str]:
    lines: List[str] = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            p = f"{prefix}.{k}" if prefix else str(k)
            lines.extend(flatten_json(v, p))
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            p = f"{prefix}[{i}]"
            lines.extend(flatten_json(v, p))
    else:
        lines.append(f"{prefix}: {obj}")
    return lines


def load_kb(paths: List[str]) -> str:
    collected: List[str] = []
    for p in paths:
        if not os.path.exists(p):
            eprint(f"Warning: KB path missing: {p}")
            continue
        try:
            if p.lower().endswith(".txt"):
                collected.append(load_text_from_file(p))
            elif p.lower().endswith(".json"):
                with open(p, "r", encoding="utf-8") as f:
                    data = json.load(f)
                collected.append("\n".join(flatten_json(data)))
            else:
                eprint(f"Warning: Unsupported KB file type (use .txt or .json): {p}")
        except Exception as e:
            eprint(f"Warning: Failed to load KB '{p}': {e}")
    return "\n".join(collected)

# ---------------------
# Local retriever (BM25-ish + cosine TF) with citations
# ---------------------
STOPWORDS = {
    "the","a","an","and","or","but","if","then","than","so","because","as","of","in","on","at","by",
    "for","to","from","with","about","into","over","after","before","between","within","without","is","are",
    "was","were","be","being","been","it","its","this","that","these","those","i","you","he","she","we","they",
    "them","his","her","their","our","my","your","me","us","do","does","did","done","can","could","should",
    "would","may","might","will","shall","have","has","had","not","no","yes","there","here","how","what",
    "which","who","whom","when","where","why"
}

_sentence_splitter = re.compile(r"(?<=[.!?])\s+")
_tokenizer = re.compile(r"[A-Za-z0-9']+")


def split_sentences(text: str) -> List[str]:
    text = text.strip()
    if not text:
        return []
    parts = _sentence_splitter.split(text)
    if len(parts) == 1:
        return [text]
    return [s.strip() for s in parts if s.strip()]


def tokenize(s: str) -> List[str]:
    return [w for w in _tokenizer.findall(s.lower()) if w not in STOPWORDS]


def tf(counter: CounterType[str]) -> Dict[str, float]:
    total = sum(counter.values()) or 1.0
    return {k: v / total for k, v in counter.items()}


def cosine(c1: Dict[str, float], c2: Dict[str, float]) -> float:
    keys = set(c1) | set(c2)
    dot = sum(c1.get(k, 0.0) * c2.get(k, 0.0) for k in keys)
    n1 = math.sqrt(sum(v * v for v in c1.values())) or 1e-12
    n2 = math.sqrt(sum(v * v for v in c2.values())) or 1e-12
    return dot / (n1 * n2)


def bm25_score(query_tokens: List[str], doc_tokens: List[str], k1: float = 1.5, b: float = 0.75,
               avgdl: float = 50.0, N: int = 1000, df_map: Optional[Dict[str,int]] = None) -> float:
    # Minimal BM25-ish scoring using rough global stats
    score = 0.0
    dl = max(1, len(doc_tokens))
    for t in query_tokens:
        if df_map is not None:
            df = max(1, df_map.get(t, 1))
        else:
            df = 10
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1.0)
        f = doc_tokens.count(t)
        numer = f * (k1 + 1)
        denom = f + k1 * (1 - b + b * (dl / avgdl))
        score += idf * (numer / denom) if denom else 0.0
    return score


@dataclass
class LocalDocQA:
    """Local, doc-only QA with hybrid scorer and citations."""
    document: str
    strict: bool = True
    max_sentences: int = 2
    min_score: float = 0.08

    def __post_init__(self) -> None:
        self.sentences: List[str] = split_sentences(self.document)
        self.sent_tf: List[Dict[str, float]] = [tf(Counter(tokenize(s))) for s in self.sentences]
        self.sent_tokens: List[List[str]] = [list(Counter(tokenize(s)).elements()) for s in self.sentences]
        # Docfreq map for BM25-ish
        df: Counter = Counter()
        for toks in self.sent_tokens:
            for t in set(toks):
                df[t] += 1
        self.df_map: Dict[str,int] = dict(df)
        self.avgdl: float = sum(len(t) for t in self.sent_tokens)/max(1,len(self.sentences))

    def score(self, question: str) -> List[Tuple[float,int]]:
        q_tokens = tokenize(question)
        if not q_tokens:
            return []
        q_tf = tf(Counter(q_tokens))
        cos_scores = [cosine(q_tf, s_tf) for s_tf in self.sent_tf]
        bm25_scores = [bm25_score(q_tokens, toks, avgdl=self.avgdl, N=max(1,len(self.sentences)), df_map=self.df_map)
                       for toks in self.sent_tokens]
        blended = []
        for i,(c,bm) in enumerate(zip(cos_scores,bm25_scores)):
            blended.append(((0.6*c) + (0.4*(bm/5.0)), i))  # normalize BM25-ish
        blended.sort(reverse=True)
        return blended

    def best_score(self, question: str) -> float:
        s = self.score(question)
        return s[0][0] if s else 0.0

    def top_sentences(self, question: str, k: int = 2) -> List[Tuple[str,int,float]]:
        ranked = self.score(question)
        out: List[Tuple[str,int,float]] = []
        for sc, idx in ranked[:k]:
            if sc >= self.min_score:
                out.append((self.sentences[idx], idx, sc))
        return out

    def ask(self, question: str) -> Dict[str, Any]:
        tops = self.top_sentences(question, self.max_sentences)
        if not tops:
            return {"answer": NOT_FOUND_LINE, "citations": [], "confidence": 0.0}
        answer = " ".join(t for (t,_,_) in tops)
        citations = [{"sentence_index": i, "text": t, "score": round(s,4)} for (t,i,s) in tops]
        conf = min(0.99, sum(s for (_,_,s) in tops)/len(tops))
        return {"answer": answer, "citations": citations, "confidence": round(conf,4)}


# --------------
# Gemini backend
# --------------
class GeminiChat:
    def __init__(self, model_name: str, api_key: str, strict: bool = True):
        if not HAS_GENAI:
            raise RuntimeError("Gemini backend requested but google.generativeai is not available.")
        genai.configure(api_key=api_key)
        system_instruction = STRICT_INSTRUCTION if strict else RELAXED_INSTRUCTION
        self.model = genai.GenerativeModel(model_name, system_instruction=system_instruction)
        self.chat = self.model.start_chat(history=[])

    def prime_with_chunks(self, chunks: List[str]) -> None:
        for i, chunk in enumerate(chunks, 1):
            preface = f"Document Chunk {i}/{len(chunks)}:\n"
            backoff = 1.0
            for attempt in range(4):
                try:
                    _ = self.chat.send_message(preface + chunk)
                    break
                except Exception as e:
                    msg = str(e)
                    if attempt < 3 and ("429" in msg or "quota" in msg.lower() or "rate" in msg.lower()):
                        time.sleep(backoff)
                        backoff *= 2
                        continue
                    raise

    def ask(self, prompt: str) -> str:
        resp = self.chat.send_message(prompt)
        out = getattr(resp, "text", None)
        if not out and hasattr(resp, "candidates"):
            try:
                out = resp.candidates[0].content.parts[0].text
            except Exception:
                out = None
        return out or "<no response>"

    def ask_with_context(self, question: str, context: str, relaxed: bool) -> str:
        guard = (
            "Answer ONLY using the provided CONTEXT. If the answer is not present, reply exactly: \n"
            f"'{NOT_FOUND_LINE}'\n"
        ) if not relaxed else (
            "Prefer the CONTEXT; if you use outside knowledge, clearly label it as 'Beyond-document note: ...'\n"
        )
        prompt = (
            f"{guard}\n\nCONTEXT:\n{context}\n\nQUESTION: {question}\n"
        )
        return self.ask(prompt)


# -------------
# Hybrid engine wrapper
# -------------
@dataclass
class HybridEngine:
    local: LocalDocQA
    gemini: Optional[GeminiChat] = None
    strict: bool = True
    escalate_threshold: float = 0.10

    def ask(self, question: str) -> Dict[str, Any]:
        local_pack = self.local.ask(question)
        best = self.local.best_score(question)

        if self.strict or self.gemini is None:
            return local_pack

        if best >= self.escalate_threshold and local_pack["answer"] != NOT_FOUND_LINE:
            return local_pack

        ctx_snips = self.local.top_sentences(question, k=4)
        context = "\n".join(t for (t,_,_) in ctx_snips) if ctx_snips else "(no strong matches in the document)"
        try:
            g_answer = self.gemini.ask_with_context(question, context, relaxed=True)
            # Include local citations for transparency
            local_pack["answer"] = g_answer
            local_pack["escalated"] = True
            return local_pack
        except Exception as e:
            eprint(f"Warning: Gemini call failed, returning local answer. Details: {e}")
            return local_pack


# -------------
# Public Tool facade
# -------------
@dataclass
class Tool:
    engine: Any

    @classmethod
    def from_sources(cls, base_text: str, kb_paths: List[str], backend: str = "hybrid",
                     model: str = "gemini-1.5-pro-latest", strict: bool = True,
                     min_score: float = 0.08, max_sentences: int = 2) -> "Tool":
        # Merge corpus
        kb_text = load_kb(kb_paths) if kb_paths else ""
        if not base_text.strip() and not kb_text.strip():
            base_text = (
                "The sky is blue. Grass is usually green. Clouds are made of tiny water droplets or ice crystals. "
                "On clear days, the blue color is due to Rayleigh scattering."
            )
        document = (base_text + "\n\n" + kb_text).strip()
        local = LocalDocQA(document=document, strict=True, min_score=min_score, max_sentences=max_sentences)

        if backend == "local":
            return cls(local)

        if backend == "gemini":
            if not HAS_GENAI or not os.getenv("GEMINI_API_KEY"):
                eprint("Warning: Gemini not available; using local backend.")
                return cls(local)
            g = GeminiChat(model, os.getenv("GEMINI_API_KEY"), strict=strict)
            chunks = chunk_text(document)
            if chunks:
                g.prime_with_chunks(chunks)
            return cls(g)

        # hybrid
        g = None
        if HAS_GENAI and os.getenv("GEMINI_API_KEY"):
            try:
                g = GeminiChat(model, os.getenv("GEMINI_API_KEY"), strict=strict)
                chunks = chunk_text(document)
                if chunks:
                    g.prime_with_chunks(chunks)
            except Exception as e:
                eprint(f"Warning: Gemini init failed: {e}")
                g = None
        engine = HybridEngine(local=local, gemini=g, strict=strict)
        return cls(engine)

    def ask(self, question: str) -> Dict[str, Any]:
        res = self.engine.ask(question)
        # Normalize to dict
        if isinstance(res, str):
            return {"answer": res, "citations": [], "confidence": None}
        return res


# ---------
# JSON-RPC server (stdin/stdout)
# ---------

def run_jsonrpc(tool: Tool) -> int:
    """Simple newline-delimited JSON-RPC-like loop.
    Input lines: {"method":"ask","params":{"question":"..."}}
    Output lines: JSON dict with result or error.
    """
    for line in sys.stdin:
        line = line.strip()
        if not line:
            continue
        try:
            req = json.loads(line)
            method = req.get("method")
            params = req.get("params", {}) or {}
            if method == "ask":
                q = params.get("question", "")
                out = tool.ask(q)
                print(json.dumps({"ok": True, "result": out}))
            else:
                print(json.dumps({"ok": False, "error": f"unknown method: {method}"}))
        except Exception as e:
            print(json.dumps({"ok": False, "error": str(e)}))
    return 0


# ---------
# CLI
# ---------
BANNER = (
    "\n=============================================================\n"
    "Harmonic Sovereign Hybrid Tool — Local-first QA (Hybrid-ready)\n"
    "• --question/--questions-file for batch\n"
    "• --jsonrpc for programmatic control\n"
    "=============================================================\n"
)

HELP_TEXT = (
    "Mode: {mode} | Backend: {backend}\n"
    "Ask away. Type 'exit' to quit (interactive).\n"
)


def build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="Harmonic Sovereign Hybrid Tool")
    p.add_argument("text_file", nargs="?", help="Path to a UTF-8 .txt to ground on")
    p.add_argument("--kb", action="append", default=[], help="Extra KB (.txt or .json); repeatable")
    p.add_argument("--backend", choices=["hybrid", "local", "gemini"], default="hybrid")
    p.add_argument("--model", default="gemini-1.5-pro-latest", help="Gemini model name if used")
    p.add_argument("--no-strict", action="store_true", help="Allow beyond-doc in hybrid/Gemini")
    p.add_argument("--min-score", type=float, default=0.08)
    p.add_argument("--max-sentences", type=int, default=2)
    p.add_argument("--question", action="append", default=[], help="Batch question (repeatable)")
    p.add_argument("--questions-file", help="File with one question per line")
    p.add_argument("--jsonrpc", action="store_true", help="Run JSON-RPC over stdin/stdout")
    p.add_argument("--self-test", action="store_true", help="Run unit tests and exit")
    return p


def read_questions_file(path: str) -> List[str]:
    qs: List[str] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                qs.append(line)
    return qs


def get_demo_questions() -> List[str]:
    """Questions guaranteed to hit the demo corpus so users see a real answer."""
    return [
        "What color is the sky?",
        "Why is the sky blue on clear days?",
    ]


# ---------
# Tests
# ---------
class ToolTests(unittest.TestCase):
    SAMPLE_DOC = (
        "The sky is blue. Grass is usually green. Clouds are made of tiny water droplets or ice crystals. "
        "On clear days, the blue color is due to Rayleigh scattering."
    )

    def setUp(self) -> None:
        self.local = LocalDocQA(self.SAMPLE_DOC, strict=True, max_sentences=2, min_score=0.05)

    def test_demo_questions_hit_demo_corpus(self):
        # Ensure our demo defaults actually return an in-doc answer
        tool = Tool.from_sources(base_text="", kb_paths=[], backend="local", strict=True)
        for q in get_demo_questions():
            out = tool.ask(q)
            self.assertNotEqual(out["answer"], NOT_FOUND_LINE)

    def test_tokenize_stopwords(self):
        toks = tokenize("What COLOR is the sky?!")
        self.assertIn("color", toks)
        self.assertIn("sky", toks)
        self.assertNotIn("is", toks)

    def test_local_answer(self):
        pack = self.local.ask("What color is the sky?")
        self.assertIn("blue", pack["answer"].lower())
        self.assertGreaterEqual(pack["confidence"], 0.05)
        self.assertTrue(pack["citations"])  # has provenance

    def test_local_not_found(self):
        pack = self.local.ask("What is the capital of France?")
        self.assertEqual(pack["answer"], NOT_FOUND_LINE)

    def test_bm25_cosine_blend(self):
        scores = self.local.score("clear blue sky")
        self.assertTrue(scores and scores[0][0] > 0.0)

    def test_flatten_json(self):
        sample = {"a": 1, "b": [2, {"c": 3}]}
        lines = flatten_json(sample)
        self.assertTrue(any("a:" in ln for ln in lines))
        self.assertTrue(any("b[0]:" in ln for ln in lines))
        self.assertTrue(any("b[1].c:" in ln for ln in lines))

    def test_main_callable(self):
        # main should be defined for the safe runner
        self.assertTrue(callable(main))


# ---------
# Program entry points
# ---------

def main() -> int:
    parser = build_arg_parser()
    args = parser.parse_args()

    if args.self_test:
        suite = unittest.defaultTestLoader.loadTestsFromTestCase(ToolTests)
        result = unittest.TextTestRunner(verbosity=2).run(suite)
        return 0 if result.wasSuccessful() else 1

    base_text = ""
    if args.text_file and os.path.exists(args.text_file):
        try:
            base_text = load_text_from_file(args.text_file)
        except Exception as e:
            eprint(f"Warning: could not read '{args.text_file}': {e}")

    kb_paths = list(args.kb)
    default_json = "/mnt/data/hodge_data_haqhml.json"
    if os.path.exists(default_json):
        kb_paths.append(default_json)

    tool = Tool.from_sources(
        base_text=base_text,
        kb_paths=kb_paths,
        backend=args.backend,
        model=args.model,
        strict=not args.no_strict,
        min_score=args.min_score,
        max_sentences=args.max_sentences,
    )

    if args.jsonrpc:
        return run_jsonrpc(tool)

    batch_qs: List[str] = list(args.question)
    if args.questions_file and os.path.exists(args.questions_file):
        try:
            batch_qs.extend(read_questions_file(args.questions_file))
        except Exception as e:
            eprint(f"Warning: could not read questions file: {e}")

    # Non-interactive by default in sandbox if no questions provided
    stdin_tty = False
    try:
        stdin_tty = sys.stdin.isatty()
    except Exception:
        stdin_tty = False

    mode = "STRICT (doc-only)" if (not args.no_strict) else "RELAXED (hybrid/Gemini may go beyond doc)"

    if batch_qs or (not stdin_tty):
        print(BANNER)
        print(HELP_TEXT.format(mode=mode, backend=args.backend))
        # If no questions were provided and we're non-interactive, use demo questions that actually match
        if not batch_qs:
            batch_qs = get_demo_questions()
        for q in batch_qs:
            out = tool.ask(q)
            print(json.dumps({"question": q, **out}, ensure_ascii=False))
        return 0

    # Interactive mode
    print(BANNER)
    print(HELP_TEXT.format(mode=mode, backend=args.backend))
    while True:
        try:
            q = input("Your question: ").strip()
            if q.lower() in {"exit", "quit", "q"}:
                print("Exiting. Bye!")
                break
            if not q:
                print("Please enter a question.")
                continue
            out = tool.ask(q)
            print(json.dumps(out, ensure_ascii=False))
        except (EOFError, KeyboardInterrupt):
            print("Exiting.")
            break
        except OSError as e:
            eprint(f"Non-interactive stdin detected ({e}); exiting interactive loop.")
            break
    return 0


# ---------
# Safe main runner
# ---------

def _safe_main_in_canvas() -> None:
    """Run main() without raising SystemExit so sandboxes don't print a traceback."""
    code = 0
    try:
        code = int(main())
    except SystemExit as e:
        try:
            code = int(getattr(e, "code", 0) or 0)
        except Exception:
            code = 0
    if sys.stdin.isatty():
        sys.exit(code)
    else:
        # Suppress noisy output unless non-zero
        if code != 0:
            print(f"[program finished with exit code {code}]")


if __name__ == "__main__":
    _safe_main_in_canvas()
